{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a369ddd1-0fb5-44b5-a6f0-cc2293f624dd",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "Ans.1\n",
    "\n",
    "Web scraping is the process of extracting data from websites using automated tools or software. It involves writing a program that can automatically extract and collect large amounts of data from multiple web pages and save it in a structured format such as a spreadsheet, database, or JSON file. Web scraping is used to collect data for a variety of purposes including market research, price monitoring, lead generation, sentiment analysis, and more.\n",
    "\n",
    "Here are three areas where web scraping is commonly used:\n",
    "\n",
    "1.E-commerce: E-commerce companies use web scraping to monitor competitors' prices, product features and descriptions. This information is used to adjust their own prices, improve their products, and stay ahead of their competitors.\n",
    "\n",
    "2.Research: Researchers use web scraping to gather data from various sources such as social media, news websites, and blogs. This data can then be used for analysis and insights on a wide range of topics such as public opinion, consumer behavior, and market trends.\n",
    "\n",
    "3.Job Aggregation: Job aggregation platforms use web scraping to collect job postings from different job boards and company websites. This allows them to display all relevant job openings in one place and provide a better user experience for job seekers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8513f-6873-47cc-817a-b3aa5cd43883",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "Ans.2\n",
    "\n",
    "There are several methods used for web scraping. Here are some of the popular ones:\n",
    "\n",
    "1.Using APIs: Some websites provide APIs (Application Programming Interfaces) that allow developers to access data in a structured and organized way. APIs provide a more reliable and efficient way to extract data from websites compared to scraping the HTML directly.           \n",
    "\n",
    "2.Parsing HTML: This involves using libraries like BeautifulSoup, lxml, or HTMLParser to parse the HTML code of a website and extract relevant data.           \n",
    " \n",
    "3.Automated browsing: Some web scraping tools use automated browsing techniques to interact with a website just like a human would. They can automatically fill out forms, click buttons, and navigate through the site to collect data.       \n",
    "\n",
    "4.DOM parsing: The Document Object Model (DOM) is the hierarchical representation of a webpage, and parsing it involves inspecting the structure and content of a webpage to extract data   . \n",
    "\n",
    "5.Regex: Regular expressions can be used to search for patterns in the HTML code and extract data based on those patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a778b4-6523-4a26-a05f-ba3840abf0e3",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Ans.3\n",
    "\n",
    "Beautiful Soup is a Python library used for web scraping purposes to pull the data out of HTML and XML files. It creates parse trees from HTML and XML documents that can be used to extract data in a hierarchical and more readable manner.\n",
    "\n",
    "Beautiful Soup provides a simple way to navigate, search, and modify the parse tree. It is easy to use and has a wide range of functionality including advanced searching and filtering capabilities.\n",
    "\n",
    "Some of the features of Beautiful Soup include:\n",
    "\n",
    "1.Parsing HTML and XML documents     \n",
    "2.Navigating parse trees using CSS selector syntax          \n",
    "3.Searching for tags and attributes         \n",
    "4.Extracting data from HTML tags and attributes              \n",
    "5.Modifying the parse tree          \n",
    "\n",
    "     \n",
    "     Beautiful Soup is widely used in web scraping because it makes it easier to extract and manipulate data from HTML and XML documents compared to other parsing libraries. Its popularity also means that there is a large community of users who can provide support and share resources like tutorials and code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934bae3-c177-4997-bd5f-24dfc25d9486",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "Ans 4.\n",
    "\n",
    "Flask is a micro web framework for Python that is used to build web applications. Flask is used in this web scraping project to create a server-side application that can handle HTTP requests and responses.\n",
    "\n",
    "In this web scraping project, Flask can be used to:\n",
    "\n",
    "1.Expose a REST API endpoint: Flask can be used to create a route that responds to HTTP requests and returns data in JSON or XML format.           \n",
    "\n",
    "2.Schedule scraping tasks: Flask can be used with third-party libraries like Celery or APScheduler to schedule scraping tasks at specific intervals.                 \n",
    "\n",
    "3.Store scraped data: Flask can be used with a database like SQLite or PostgreSQL to store the scraped data. This makes it easier to access and analyze the data later.              \n",
    "\n",
    "4.Provide an interface for visualizing the data: Flask can be used with front-end libraries like React or Angular to create interactive data visualizations.             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a629c-9a39-48ef-ba57-5f5ef0ab9720",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "Ans.5\n",
    "\n",
    "Code pipeline and  AWS Elastic Beanstalk is the Aws services used in this project of Web scrapping.\n",
    "\n",
    "\n",
    "Code Pipeline:\n",
    "            \n",
    "            i.This service is used to connect github repo to beanstack environment.\n",
    "           ii.CodePipeline automates the build, test, and deploy phases of your release process every time there\n",
    "             is a code change, based on the release model you define.\n",
    "\n",
    "\n",
    "AWS Elastic Beanstalk:\n",
    "\n",
    "        \n",
    "            Elastic Beanstalk provides a fully managed platform that allows developers to quickly deploy applications built with popular web frameworks such as Node.js, Java, Python, Ruby, PHP, and Go. It takes care of the underlying infrastructure, including provisioning servers, configuring load balancing and auto-scaling, and monitoring application health.\n",
    "\n",
    "            Elastic Beanstalk automatically handles deployment details such as capacity provisioning, load balancing, scaling, and application health monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
